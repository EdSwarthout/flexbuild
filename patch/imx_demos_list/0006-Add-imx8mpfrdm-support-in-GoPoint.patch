From e80f755e66d3cdb5d292fa980049bec0a3eaab8b Mon Sep 17 00:00:00 2001
From: Andy Tang <andy.tang@nxp.com>
Date: Thu, 8 May 2025 15:48:39 +0800
Subject: [PATCH] Add imx8mpfrdm support in GoPoint

Signed-off-by: Andy Tang <andy.tang@nxp.com>
---
 demos.json                                   |  60 +++----
 scripts/machine_learning/face_recognition.py | 165 +++++++++++--------
 2 files changed, 124 insertions(+), 101 deletions(-)

diff --git a/demos.json b/demos.json
index f36d060..738b249 100644
--- a/demos.json
+++ b/demos.json
@@ -4,7 +4,7 @@
              "name": "Image Classification",
              "id": "obj_class_nn",
              "executable": "python3 /opt/gopoint-apps/scripts/machine_learning/nnstreamer/classification/image_classification.py && killall example_classification_mobilenet_v1_tflite example_classification_two_cameras_tflite",
-             "compatible": "imx8mp, imx93, imx8mm, imx8qmmek, imx95, imx93frdm",
+             "compatible": "imx8mp, imx8mpfrdm, imx93, imx8mm, imx8qmmek, imx95, imx93frdm",
              "screenshot": "image_classification.jpg",
              "icon": "ml-icon.svg",
              "description": "Image classification example using NNStreamer. Image classification is an ML task that attempts to comprehend an entire image as a whole. The goal is to classify the image by assigning it to a specific label. Typically, it refers to images in which only one object appears and is analyzed. An internet connection may be required."
@@ -12,7 +12,7 @@
              "name": "Object Detection",
              "id": "obj_detect_nn",
              "executable": "python3 /opt/gopoint-apps/scripts/machine_learning/nnstreamer/detection/object_detection.py && killall example_detection_mobilenet_ssd_v2_tflite",
-             "compatible": "imx8mp, imx93, imx8mm, imx8qmmek, imx95, imx93frdm",
+             "compatible": "imx8mp, imx8mpfrdm, imx93, imx8mm, imx8qmmek, imx95, imx93frdm",
              "screenshot": "ml_detect.jpg",
              "icon": "ml-icon.svg",
              "description": "Object detection example using NNStreamer. Object detection is the ML task that detects instances of objects of a certain class within an image. A bounding box and a class label are found for each detected object. An internet connection may be required."
@@ -20,7 +20,7 @@
              "name": "Pose Estimation",
              "id": "pose_detect_nn",
              "executable": "python3 /opt/gopoint-apps/scripts/machine_learning/nnstreamer/pose/pose_estimation.py && killall example_pose_movenet_tflite",
-             "compatible": "imx8mp, imx8mm, imx8qmmek, imx93, imx95",
+             "compatible": "imx8mp, imx8mpfrdm, imx8mm, imx8qmmek, imx93, imx95",
              "screenshot": "pose-estimation.jpg",
              "icon": "ml-icon.svg",
              "description": "Pose estimation example using NNStreamer. The goal of pose estimation is to detect the position and orientation of a person or object. In human pose estimation, this is usually done with specific keypoints such as hands, head, legs, etc. An internet connection may be required."
@@ -28,7 +28,7 @@
              "name": "ML Gateway",
              "id": "ml_gate_nn",
              "executable": "python3 /opt/gopoint-apps/scripts/machine_learning/ml_gateway/ml_gateway.py",
-             "compatible": "imx8mp, imx8mm, imx93",
+             "compatible": "imx8mp, imx8mpfrdm, imx8mm, imx93",
              "screenshot": "ml_gateway.jpg",
              "icon": "ml-icon.svg",
              "description": "ML Gateway easily configures the i.MX 8M Plus and i.MX 93 EVKs as machine learning accelerator servers and allows resource-constrained MPU systems (clients) without NPUs to connect and run ML inference. This is currently enabled for i.MX 8M Mini on the client side."
@@ -36,7 +36,7 @@
              "name": "Selfie Segmenter",
              "id": "selfie_nn",
              "executable": "python3 /opt/gopoint-apps/scripts/machine_learning/selfie_segmenter/selfie_segmenter.py",
-             "compatible": "imx8mp, imx93, imx93frdm",
+             "compatible": "imx8mp, imx8mpfrdm, imx93, imx93frdm",
              "screenshot": "selfie_segmenter.jpg",
              "icon": "ml-icon.svg",
              "description": "Selfie Segmenter showcases the ML capabilities of i.MX 8M Plus and i.MX 93 by using the NPU to accelerate an instance segmentation model. This model lets you segment the portrait of a person and can be used to replace or modify the background of an image. An internet connection is required."
@@ -44,7 +44,7 @@
               "name": "i.MX Smart Fitness",
               "id": "imx-smart-fitness",
               "executable": "python3 /opt/gopoint-apps/scripts/machine_learning/imx_smart_fitness/imx_smart_fitness.py",
-              "compatible": "imx8mp, imx93, imx93frdm",
+              "compatible": "imx8mp, imx8mpfrdm, imx93, imx93frdm",
               "screenshot": "imx-smart-fitness.jpg",
               "icon": "ml-icon.svg",
               "description": "i.MX Smart Fitness showcases the i.MX' Machine Learning capabilities by using an NPU to accelerate two Deep Learning vision-based models. Together, these models detect a person present in the scene and predict 33 3D-keypoints to generate a complete body landmark, known as pose estimation. From the pose estimation, a K-NN pose classifier classifies two different body poses: 'Squat-Down' and 'Squat-Up'. The application tracks the 'squats' fitness exercise and the repetition counter is set to 12 repetitions in an infinite loop."
@@ -52,7 +52,7 @@
         "TFLite":[{
             "name": "ML Benchmark",
             "executable": "python3 /opt/gopoint-apps/scripts/machine_learning/ml_benchmark/ml_benchmark.py",
-            "compatible": "imx8mp, imx93, imx95, imx93frdm",
+            "compatible": "imx8mp, imx8mpfrdm, imx93, imx95, imx93frdm",
             "screenshot": "ml_benchmark.jpg",
             "icon": "speedometer.svg",
             "description": "This tool allows to easily compare the performance of TensorFlow Lite models running on CPU (Cortex-A) and NPU. The tool works on i.MX 93 and i.MX 8M Plus."
@@ -60,8 +60,8 @@
         "OpenCV":[{
             "name": "Face Recognition",
             "id": "face_recog",
-            "executable": "python3 /opt/gopoint-apps/scripts/machine_learning/face_recognition.py --gui=0 --camera=3",
-            "compatible": "imx8mp",
+            "executable": "python3 /opt/gopoint-apps/scripts/machine_learning/face_recognition.py",
+            "compatible": "imx8mp, imx8mpfrdm",
             "screenshot": "face_recognition.jpg",
             "icon": "ml-icon.svg",
             "description": "An OpenCV application example of how to use machine learning to recognize faces. The user can save multiple profiles and the application will recognize the identity of each person by their names. An internet connection is required."
@@ -69,7 +69,7 @@
             "name": "DMS",
             "id": "dms",
             "executable": "python3 /opt/gopoint-apps/scripts/machine_learning/dms/launcher.py",
-            "compatible": "imx8mp, imx93, imx93frdm",
+            "compatible": "imx8mp, imx8mpfrdm, imx93, imx93frdm",
             "screenshot": "dms.jpg",
             "icon": "ml-icon.svg",
             "description": "An example over how to implement a Driver Monitoring System (DMS) using the NPU. An internet connection is required."
@@ -101,7 +101,7 @@
             "source": "",
             "icon": "multimedia-icon.svg",
             "screenshot": "gst_test_src_screenshot.jpg",
-            "compatible": "imx7ulp, imx8ulp, imx8qxpc0mek, imx8qmmek, imx8mq, imx8mm, imx8mn, imx8mp, imx93, imx93frdm",
+            "compatible": "imx7ulp, imx8ulp, imx8qxpc0mek, imx8qmmek, imx8mq, imx8mm, imx8mn, imx8mp, imx8mpfrdm, imx93, imx93frdm",
             "description": "This is a simple demo utility that allows users to play back video captured on a camera or a test source."
         },{
             "name": "Camera using VPU",
@@ -110,7 +110,7 @@
             "source": "",
             "icon": "multimedia-icon.svg",
             "screenshot": "camera-vpu.jpg",
-            "compatible": "imx8mp",
+            "compatible": "imx8mp, imx8mpfrdm",
             "description": "This is a GStreamer pipeline able to create a camera preview example using VPU to encode and decode the image."
         },{
             "name": "Video To Texture Demo",
@@ -128,7 +128,7 @@
             "source": "",
             "icon": "multimedia-icon.svg",
             "screenshot": "two-way-video-streaming.jpg",
-            "compatible": "imx8mp, imx8mm",
+            "compatible": "imx8mp, imx8mpfrdm, imx8mm",
             "description": "Allows user to implement a two way video streaming demo that displays video encode and decode capabilities between i.MX devices in local network."
         },{
             "name": "Multi Cameras Preview",
@@ -137,7 +137,7 @@
             "source": "",
             "icon": "multimedia-icon.svg",
             "screenshot": "multi_cameras.jpg",
-            "compatible": "imx8mp",
+            "compatible": "imx8mp, imx8mpfrdm",
             "description": "This is a GStreamer pipeline able to create a camera preview example using a Basler/OS08A20 camera and an OV5640 camera simultaneously."
         }],
         "ISP":[{
@@ -147,7 +147,7 @@
             "source": "",
             "icon": "multimedia-icon.svg",
             "screenshot": "isp_demo.png",
-            "compatible": "imx8mp",
+            "compatible": "imx8mp, imx8mpfrdm",
             "description": "This program opens a GStreamer pipeline and allows the user to change various parameters of the ISP in real time. This example application works with Basler and OS08A20 cameras."
         },{
             "name": "Video Dump",
@@ -206,7 +206,7 @@
             "source": "",
             "icon": "voice-control.svg",
             "screenshot": "smart-kitchen-screenshot.jpg",
-            "compatible": "imx8mm, imx8mp, imx93, imx93frdm",
+            "compatible": "imx8mm, imx8mp, imx8mpfrdm, imx93, imx93frdm",
             "description": "This application simulates a smart kitchen controlled by voice commands using NXP's Voice Intelligent Technology (VIT). How to use: First say a wakeword to select a kitchen's item (hood, oven or aircon) and then say one of the item's available commands (e.g. \"Hey hood, light on\"). WakeWords supported are HEY HOOD, HEY OVEN, HEY AIRCON. Global Commands are ENTER, EXIT, RUN DEMO, STOP DEMO. Hood commands are FAN OFF, FAN ON, FAN LOW, FAN HIGH, LIGHT OFF, LIGHT ON. Aircon commands are DRY MODE, COOL MODE, FAN MODE SWING OFF, SWING ON, FAN LOW, FAN HIGH. Oven commands are CLOSE DOOR, OPEN DOOR. The item's functions can also be activated by clicking on the item's controls using a mouse or touchscreen."
         },
         {
@@ -216,7 +216,7 @@
              "source": "",
              "icon": "ebike-vit-icon.svg",
              "screenshot": "ebike-vit-screenshot.jpg",
-             "compatible": "imx8mm, imx8mp, imx93, imx93frdm",
+             "compatible": "imx8mm, imx8mp, imx8mpfrdm, imx93, imx93frdm",
              "description": "This application simulates a ebike controlled by voice commands using NXP's Voice Intelligent Technology (VIT). First say a wakeword to wake up ebike and then say one of the available commands.The supported wake words are HEY NXP, HEY E-Bike. Global Commands are NEXT PAGE, LAST PAGE, RUN DEMO, STOP DEMO. The page can also be switched by clicking on the UI using a mouse or touchscreen."
 	}]
     }]
@@ -255,7 +255,7 @@
             "source": "",
             "icon": "graphics-icon.svg",
             "screenshot": "tiger_2d.jpg",
-            "compatible": "imx7ulp, imx8mq, imx8mm, imx8mn, imx8qxpc0mek, imx8qmmek, imx8mp, imx8ulp",
+            "compatible": "imx7ulp, imx8mq, imx8mm, imx8mn, imx8qxpc0mek, imx8qmmek, imx8mp, imx8mpfrdm, imx8ulp",
             "description": "Vivante Tiger G2D, this demo shows a vector image being rotated and scaled using OpenVG."
         }],
         "GLES2":[{
@@ -265,7 +265,7 @@
             "source": "",
             "icon": "graphics-icon.svg",
             "screenshot": "vivante_vv_laucher.jpg",
-            "compatible": "imx7ulp, imx8qxpc0mek, imx8qmmek, imx8mp, imx8ulp",
+            "compatible": "imx7ulp, imx8qxpc0mek, imx8qmmek, imx8mp, imx8mpfrdm, imx8ulp",
             "description": "Vivante launcher demo."
         },
         {
@@ -295,7 +295,7 @@
             "source": "",
             "icon": "graphics-icon.svg",
             "screenshot": "bloom.jpg",
-            "compatible": "imx8mq, imx8mm, imx8mn, imx8qxpc0mek, imx8qmmek, imx8mp, imx95",
+            "compatible": "imx8mq, imx8mm, imx8mn, imx8qxpc0mek, imx8qmmek, imx8mp, imx8mpfrdm, imx95",
             "description": "An example of how to create a bloom effect. The idea is not to create the most accurate bloom, but something that is fairly fast to render. Instead of increasing the kernal size to get a good blur we do a fairly fast approximation by downscaling the original image to multiple smaller render-targets and then blurring these using a relative small kernel and then finally rescaling the result to the original size."
         },
         {
@@ -315,7 +315,7 @@
             "source": "",
             "icon": "graphics-icon.svg",
             "screenshot": "blur.jpg",
-            "compatible": "imx8mq, imx8mm, imx8mn, imx8qxpc0mek, imx8qmmek, imx8mp, imx95",
+            "compatible": "imx8mq, imx8mm, imx8mn, imx8qxpc0mek, imx8qmmek, imx8mp, imx8mpfrdm, imx95",
             "description": "Uses the two pass linear technique and further reduces the bandwidth requirement by downscaling the 'source image' to 1/4 its size (1/2w x 1/2h) before applying the blur and and then upscaling the blurred image to provide the final image. This works well for large kernel sizes and relatively high sigma's but the downscaling produces visible artifacts with low sigma's."
         },
         {
@@ -335,7 +335,7 @@
             "source": "",
             "icon": "graphics-icon.svg",
             "screenshot": "eightlayerblend.jpg",
-            "compatible": "imx8mq, imx8mm, imx8mn, imx8qxpc0mek, imx8qmmek, imx8mp, imx95",
+            "compatible": "imx8mq, imx8mm, imx8mn, imx8qxpc0mek, imx8qmmek, imx8mp, imx8mpfrdm, imx95",
             "description": "Creates a simple parallax scrolling effect by blending eight 32 bit per pixel 1080p layers on top of each other. This is not the most optimal way to do it as it uses eight passes. But it does provide a good example of the worst case bandwidth use for the operation. The demo was created to compare GLES to the G2D eight blend blit functionality."
         },
         {
@@ -355,7 +355,7 @@
             "source": "",
             "icon": "graphics-icon.svg",
             "screenshot": "fractalshader.jpg",
-            "compatible": "imx8mq, imx8mm, imx8mn, imx8qxpc0mek, imx8qmmek, imx8mp, imx95",
+            "compatible": "imx8mq, imx8mm, imx8mn, imx8qxpc0mek, imx8qmmek, imx8mp, imx8mpfrdm, imx95",
             "description": "Can render both the julia and mandelbrot set using a fragment shader. This demo was used to demonstrates GPU shader performance by using up roughly 515 instructions to render each fragment while generating the julia set. It uses no textures, has no overdraw and has a minimal bandwidth requirement."
         },
         {
@@ -385,7 +385,7 @@
             "source": "",
             "icon": "graphics-icon.svg",
             "screenshot": "linebuilder101.jpg",
-            "compatible": "imx8mq, imx8mm, imx8mn, imx8qxpc0mek, imx8qmmek, imx8mp, imx95",
+            "compatible": "imx8mq, imx8mm, imx8mn, imx8qxpc0mek, imx8qmmek, imx8mp, imx8mpfrdm, imx95",
             "description": "A simple example of dynamic line rendering using the LineBuilder helper class. The line builder has 'Add' methods for most FslBase.Math classes like BoundingBox, BoundingSphere, BoundingFrustrum, Ray, etc."
         },
         {
@@ -405,7 +405,7 @@
             "source": "",
             "icon": "graphics-icon.svg",
             "screenshot": "modelloaderbasics.jpg",
-            "compatible": "imx8mq, imx8mm, imx8mn, imx8qxpc0mek, imx8qmmek, imx8mp, imx95",
+            "compatible": "imx8mq, imx8mm, imx8mn, imx8qxpc0mek, imx8qmmek, imx8mp, imx8mpfrdm, imx95",
             "description": "Demonstrates how to use the FslSceneImporter and Assimp to load a scene and render it using OpenGLES2. The model is rendered using a simple per pixel directional light shader."
         },
         {
@@ -425,7 +425,7 @@
             "source": "",
             "icon": "graphics-icon.svg",
             "screenshot": "s03_transform.jpg",
-            "compatible": "imx8mq, imx8mm, imx8mn, imx8qxpc0mek, imx8qmmek, imx8mp, imx95",
+            "compatible": "imx8mq, imx8mm, imx8mn, imx8qxpc0mek, imx8qmmek, imx8mp, imx8mpfrdm, imx95",
             "description": "Renders a animated vertex colored triangle. This shows how to modify the model matrix to rotate a triangle and how to utilize demoTime.DeltaTime to do frame rate independent animation."
         },
         {
@@ -445,7 +445,7 @@
             "source": "",
             "icon": "graphics-icon.svg",
             "screenshot": "s04_projection.jpg",
-            "compatible": "imx8mq, imx8mm, imx8mn, imx8qxpc0mek, imx8qmmek, imx8mp, imx95",
+            "compatible": "imx8mq, imx8mm, imx8mn, imx8qxpc0mek, imx8qmmek, imx8mp, imx8mpfrdm, imx95",
             "description": "This example shows how to: - Build a perspective projection matrix - Render two simple 3d models using frame rate independent animation."
         },
         {
@@ -465,7 +465,7 @@
             "source": "",
             "icon": "graphics-icon.svg",
             "screenshot": "s06_texturing.jpg",
-            "compatible": "imx8mq, imx8mm, imx8mn, imx8qxpc0mek, imx8qmmek, imx8mp, imx95",
+            "compatible": "imx8mq, imx8mm, imx8mn, imx8qxpc0mek, imx8qmmek, imx8mp, imx8mpfrdm, imx95",
             "description": "This example shows how to use the Texture class to use a texture in a cube. It also shows you how to use the ContentManager service to load a 'png' file from the Content directory into a bitmap utility class which is then used to create an OpenGL ES texture."
         },
         {
@@ -485,7 +485,7 @@
             "source": "",
             "icon": "graphics-icon.svg",
             "screenshot": "s07_environmentmapping.jpg",
-            "compatible": "imx8mq, imx8mm, imx8mn, imx8qxpc0mek, imx8qmmek, imx8mp, imx95",
+            "compatible": "imx8mq, imx8mm, imx8mn, imx8qxpc0mek, imx8qmmek, imx8mp, imx8mpfrdm, imx95",
             "description": "This sample shows how to use a cubemap texture to simulate a reflective material. It also shows you how to use the ContentManager service to load a 'dds' file from the Content directory into a Texture utility class which is then used to create an OpenGL ES cubemap texture."
         },
         {
@@ -505,7 +505,7 @@
             "source": "",
             "icon": "graphics-icon.svg",
             "screenshot": "s08_environmentmappingrefraction.jpg",
-            "compatible": "imx8mq, imx8mm, imx8mn, imx8qxpc0mek, imx8qmmek, imx8mp, imx95",
+            "compatible": "imx8mq, imx8mm, imx8mn, imx8qxpc0mek, imx8qmmek, imx8mp, imx8mpfrdm, imx95",
             "description": "This sample is a variation from the previous sample, again, a cubemap texture is used, but this time instead of simulating a reflective material a refractive material is simulated. It also shows you how to use the ContentManager service to load a 'dds' file from the Content directory into a Texture utility class which is then used to create an OpenGL ES cubemap texture."
         },
         {
diff --git a/scripts/machine_learning/face_recognition.py b/scripts/machine_learning/face_recognition.py
index d8546b7..a108918 100644
--- a/scripts/machine_learning/face_recognition.py
+++ b/scripts/machine_learning/face_recognition.py
@@ -22,7 +22,7 @@ import tflite_runtime.interpreter as tflite
 import numpy as np
 
 import gi
-
+from gi.repository import GObject
 gi.require_version("Gtk", "3.0")
 gi.require_version("Gst", "1.0")
 from gi.repository import Gtk, Gst, GLib, Gio
@@ -63,6 +63,49 @@ class FaceDemo:
         self.detect_time = None
         self.recog_time = None
         self.write_time = False
+        self.cam_pipeline = None
+        self.last_frame = None
+        self.cv_window_created = False
+        self._face_window = None
+        self._registration_active = False
+        self._face_window_lock = threading.Lock()
+
+    def _create_opencv_window(self):
+        if not self.cv_window_created:
+            cv2.namedWindow("i.MX Face Recognition Demo", cv2.WINDOW_NORMAL)
+            cv2.resizeWindow("i.MX Face Recognition Demo", self.width, self.height)
+            self.cv_window_created = True
+    def _start_camera_pipeline(self, cam):
+        if cam == "fake":
+            self.cam_pipeline = cv2.VideoCapture(
+                "videotestsrc ! imxvideoconvert_g2d ! "
+                "video/x-raw,format=RGBA,width=" + str(self.width) + ",height=" + str(self.height) +
+                " ! videoconvert ! appsink"
+            )
+        else:
+            self.cam_pipeline = cv2.VideoCapture(
+                "v4l2src device=" + cam + " ! imxvideoconvert_g2d ! "
+                "video/x-raw,format=RGBA,width=" + str(self.width) + ",height=" + str(self.height) +
+                " ! videoconvert ! appsink"
+            )
+
+    def _process_frames(self):
+        while True:
+            status, org_img = self.cam_pipeline.read()
+            if status:
+                mod_img = self.process_frame(org_img)
+                cv2.imshow("i.MX Face Recognition Demo", mod_img)
+                cv2.waitKey(1)
+
+    def _update_opencv_window(self):
+        if self.cam_pipeline and self.cam_pipeline.isOpened():
+            status, org_img = self.cam_pipeline.read()
+            if status:
+                self.last_frame = self.process_frame(org_img)
+                cv2.imshow("i.MX Face Recognition Demo", self.last_frame)
+                cv2.waitKey(1)
+            return True
+        return False
 
     def start(self, backend, cam):
         """Starts the camera and sets up the inference engine"""
@@ -75,64 +118,16 @@ class FaceDemo:
             self.height = 1080
         if GUI:
             GLib.idle_add(MAIN_WINDOW.status_bar.set_text, "Starting cameras...")
-        if cam == "fake":
-            cam_pipeline = cv2.VideoCapture(
-                "videotestsrc ! imxvideoconvert_g2d ! "
-                "video/x-raw,format=RGBA,width="
-                + str(self.width)
-                + ",height="
-                + str(self.height)
-                + " ! "
-                + "videoconvert ! appsink"
-            )
-        else:
-            cam_pipeline = cv2.VideoCapture(
-                "v4l2src device=" + cam + " ! imxvideoconvert_g2d ! "
-                "video/x-raw,format=RGBA,width="
-                + str(self.width)
-                + ",height="
-                + str(self.height)
-                + " ! "
-                + "videoconvert ! appsink"
-            )
+
+        self._start_camera_pipeline(cam)
+
         if GUI:
             GLib.idle_add(MAIN_WINDOW.destroy)
             self.options_window = OptionsWindow()
             GLib.idle_add(self.options_window.show_all)
-            GLib.idle_add(cv2.namedWindow, "i.MX Face Recognition Demo")
-        elif OUTPUT:
-            cv2.namedWindow("i.MX Face Recognition Demo", cv2.WND_PROP_FULLSCREEN)
-            cv2.setWindowProperty(
-                "i.MX Face Recognition Demo",
-                cv2.WND_PROP_FULLSCREEN,
-                cv2.WINDOW_FULLSCREEN,
-            )
-        status, org_img = cam_pipeline.read()
-        while status:
-            mod_img = self.process_frame(org_img)
-            if self.write_time:
-                overall_time = time.perf_counter() - overall_time
-                times = self.get_timings(overall_time)
-            else:
-                times = ["N/A", "N/A"]
-            cv2.putText(
-                mod_img,
-                "Overall time: " + times[0] + " IPS (" + times[1] + " ms)",
-                (5, self.height - 10),
-                cv2.FONT_HERSHEY_SIMPLEX,
-                0.75,
-                (255, 255, 255),
-                2,
-            )
-            if GUI:
-                GLib.idle_add(cv2.imshow, "i.MX Face Recognition Demo", mod_img)
-            else:
-                if OUTPUT:
-                    cv2.imshow("i.MX Face Recognition Demo", mod_img)
-                    cv2.waitKey(1)
-            status, org_img = cam_pipeline.read()
-            self.write_time = True
-            overall_time = time.perf_counter()
+            GLib.timeout_add(50, self._update_opencv_window)
+        else:
+            self._process_frames()
 
     def setup_inferences(self, backend):
         """Sets up the inference engines"""
@@ -258,6 +253,7 @@ class FaceDemo:
             self.write_time = False
             if GUI:
                 self.options_window.unlock_controls()
+
         detect_time = self.get_timings(self.detect_time)
         cv2.putText(
             frame,
@@ -394,14 +390,38 @@ class FaceDemo:
         return time.perf_counter() - start
 
     def register_face(self, face_mask):
-        """Registers a new face"""
-        face_window = FaceWindow()
-        GLib.idle_add(face_window.show_all)
-        while face_window.working:
-            time.sleep(0.1)
-        if face_window.named_face is not None:
-            self.registered_faces.append([face_window.named_face, face_mask])
-        GLib.idle_add(face_window.destroy)
+        with self._face_window_lock:
+            if self._registration_active:
+                return
+            self._registration_active = True
+            self._face_data = face_mask
+
+        GLib.idle_add(self._create_face_window)
+
+    def _create_face_window(self):
+        with self._face_window_lock:
+            if self._face_window is None:
+                self._face_window = FaceWindow()
+                self._face_window.connect("register-complete", self._on_register_complete)
+                self._face_window.connect("skip-register", self._on_skip_register)
+                self._face_window.connect("delete-event", lambda w,e: w.hide_on_delete())
+            self._face_window.show_all()
+
+    def _on_register_complete(self, window, name):
+        with self._face_window_lock:
+            self.registered_faces.append([name, self._face_data])
+            self._cleanup()
+
+    def _on_skip_register(self, window):
+        with self._face_window_lock:
+            self._cleanup()
+
+    def _cleanup(self):
+        if self._face_window:
+            self._face_window.destroy()
+        self._face_window = None
+        self._registration_active = False
+
 
     def register_face_cli(self, face_mask):
         """Registers a new face with command line"""
@@ -625,6 +645,10 @@ class OptionsWindow(Gtk.Window):
 
 class FaceWindow(Gtk.Window):
     """GUI for users to register face"""
+    __gsignals__ = {
+        'register-complete': (GObject.SignalFlags.RUN_LAST, None, (str,)),
+        'skip-register': (GObject.SignalFlags.RUN_LAST, None, ())
+    }
 
     def __init__(self):
         """Creates GUI elements for window"""
@@ -671,18 +695,17 @@ class FaceWindow(Gtk.Window):
 
         self.add(main_grid)
 
-    def register_face(self, widget):
-        """Registers a face"""
-        self.name_box.set_sensitive(False)
-        self.add_button.set_sensitive(False)
+    def register_face(self, button):
         self.named_face = self.name_box.get_text()
         self.working = False
+        self.emit("register-complete", self.named_face)
+        self.hide()
 
-    def go_back(self, widget):
-        """Exits without registering"""
-        self.name_box.set_sensitive(False)
-        self.add_button.set_sensitive(False)
+    def go_back(self, button):
+        self.named_face = None
         self.working = False
+        self.emit("skip-register")
+        self.hide()
 
 
 if __name__ == "__main__":
-- 
2.43.0

